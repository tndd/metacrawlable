# **ğŸ•·ï¸ MetaCrawlable - Mock Web Servers for Adversarial Crawler Testing**

This document describes, in exhaustive and precise detail, the structure, purpose, components, and implementation requirements of theÂ **MetaCrawlable**Â project. This environment is designed to simulate realistic and adversarial web environments that will allow developers to rigorously test and benchmark their web crawlers.

All instructions should be interpretedÂ **literally and without assumption**. If any ambiguity is detected, halt processing and request clarification.

---

## **ğŸ”° Project Overview**

**MetaCrawlable**Â is a testbed composed of multiple independent mock websites, all hosted under a single Next.js project (using the App Router structure). Each mock website is implemented as a self-contained route, simulating specific challenges that crawlers may face in real-world hostile web environments.

This includes, but is not limited to:

- Dynamic HTML content
- JavaScript-only rendering
- Sitemap inconsistencies
- Meta tag misinformation
- Robots.txt traps
- Anti-bot middleware

These areÂ **not optional**Â features. Each one must be implemented faithfully and independently.

---

## **ğŸ¯ Purpose**

1. Allow developers to locally test scrapers and crawlers without hitting real production sites.
2. Provide realistic obstacles that represent common and advanced anti-crawling strategies.
3. Function as a baseline for crawler robustness testing, ML-based scraping evaluation, and automated bot design.

This project should be able to replicate or exceed the complexity of most real-world sites. If a crawler performs well here, it is expected to succeed in production.

---

# **ğŸ—ï¸ Project Structure**

The Next.js app must reside in the following folder:

```
mock/crawled_server/
```

All development must occur inside this directory.

Subdirectories and key files:

```
mock/crawled_server/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ dynamic/
â”‚   â”œâ”€â”€ client-only/
â”‚   â”œâ”€â”€ map/
â”‚   â”œâ”€â”€ anti-bot/
â”‚   â”œâ”€â”€ trap/[slug]/
â”‚   â”œâ”€â”€ trap-broken/
â”‚   â”œâ”€â”€ meta-fake/
â”‚   â”œâ”€â”€ no-sitemap/
â”‚   â”œâ”€â”€ partial-map/
â”‚   â””â”€â”€ layout.tsx
â”œâ”€â”€ middleware.ts         # Required for anti-bot User-Agent detection
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ robots.txt         # Multiple versions may be conditionally served
â”‚   â””â”€â”€ sitemap.xml        # Must include both complete and corrupted examples
â”œâ”€â”€ next.config.js         # May include rewrites and middleware conditions
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md              # This file

```

---

## **ğŸ§© Mock Site Summary**

Each subdirectory underÂ `app/`Â represents aÂ **mock site**. These are not pages, but full adversarial test environments. Each must include aÂ `page.tsx`Â file and conform to the following specifications:

| Site Name      | Route Path     | Required Behavior                                                               |
| -------------- | -------------- | ------------------------------------------------------------------------------- |
| `StaticLand`   | `/static`      | Plain HTML site with hardcoded internal links. Fully crawlable without JS.      |
| `DynamicMaze`  | `/dynamic`     | UsesÂ `getServerSideProps()`Â to randomize content on each request.               |
| `ClientShadow` | `/client-only` | Initial HTML is empty. Content appearsÂ **only after JavaScript execution**.     |
| `MapTown`      | `/map`         | Displays a map via Google Maps or OpenStreetMap JS API. Hidden without JS.      |
| `BotWarden`    | `/anti-bot`    | Middleware blocks or alters response for specific User-Agents.                  |
| `LinkSpiral`   | `/trap/[slug]` | Generates recursive links dynamically. Infinite depth possible.                 |
| `BrokenWeb`    | `/trap-broken` | Pages referenced in sitemap do not exist or return 404.                         |
| `MetaLie`      | `/meta-fake`   | Meta title and description differ from visible content.                         |
| `NoMapZone`    | `/no-sitemap`  | Neither sitemap.xml nor robots.txt is served. Site must be discovered manually. |
| `HalfMapSite`  | `/partial-map` | Sitemap.xml exists but omits many live pages. Misleads crawling heuristics.     |

Each of the above sitesÂ **must be implemented completely**Â and serve distinguishable HTML structures for testing parser adaptability.

---

## **âš”ï¸ Anti-Crawler Mechanisms (Per Site)**

| Feature                  | Applies To    | Implementation Requirement                                              |
| ------------------------ | ------------- | ----------------------------------------------------------------------- |
| JS-only content          | `client-only` | UseÂ `useEffect()`Â to render content post-load. Empty SSR output.        |
| SSR + DOM randomization  | `dynamic`     | Random number generation + DOM reshuffling on each load.                |
| Bot middleware           | `anti-bot`    | UseÂ `middleware.ts`Â to inspect headers and block known bot UAs.         |
| sitemap poisoning        | `trap-broken` | Include non-existent pages in sitemap.xml intentionally.                |
| recursive link trap      | `trap/[slug]` | Slugs should link to more slugs, forming an infinite navigational loop. |
| metadata mismatch        | `meta-fake`   | HTML content must differ drastically from head metadata.                |
| robots.txt inconsistency | varies        | Some paths explicitly disallowed; others not mentioned at all.          |

Any deviation from these behaviors must be considered aÂ **critical error**.

---

## **ğŸ—ºï¸ Sitemap & Robots.txt Rules**

You must implement the following sitemap/robots logic:

| Site Name      | sitemap.xml | robots.txt  | Expected Outcome                                                         |
| -------------- | ----------- | ----------- | ------------------------------------------------------------------------ |
| `StaticLand`   | âœ… Complete | âœ… Allow    | Fully visible and crawlable                                              |
| `DynamicMaze`  | âŒ None     | âœ… Disallow | Excluded from indexing by rule                                           |
| `ClientShadow` | âœ… Present  | âŒ None     | Sitemap listed but content is invisible without JS                       |
| `BotWarden`    | âŒ None     | âŒ Disallow | Crawler banned; no site map; dynamic rejection via middleware            |
| `BrokenWeb`    | âœ… Present  | âœ… Allow    | Sitemap links lead to 404s                                               |
| `HalfMapSite`  | âœ… Partial  | âœ… Allow    | Only partial site listed, requires link traversal to find full structure |
| `NoMapZone`    | âŒ None     | âŒ None     | Crawler must discover all pages organically                              |

---

# **ğŸ§©** Details of Each Mock Site

Each mock site listed below is designed to simulate a distinct real-world web environment, either in terms of technical complexity, crawler evasion strategy, or unusual behavior.

---

## **ğŸ”¹ StaticLand**

**Purpose:**Â A baseline testbed for evaluating crawler performance in semantically rich, fully static HTML environments. This confirms whether a crawler can parse standard markup and traverse internal links.

**Real-world Model:**Â Static blogs using Jekyll, Hugo, GitHub Pages, etc.

**Route:**Â `/static`

**Structure:**

```
/static                  â† Homepage with article list and tag links
/static/articles/[id]   â† Individual articles (at least 30+ pages)
/static/tags/[tag]      â† Article listing by tag
```

**Key Features:**

- All pages are statically generated usingÂ `generateStaticParams`
- `<title>`Â andÂ `<meta>`Â tags provided for each article
- Semantic elements:Â `<h1>`,Â `<h2>`,Â `<p>`,Â `<ul>`,Â `<a>`, etc.
- Each article links to 3â€“5 related articles
- Tags used across articles (1â€“3 per post)
- GlobalÂ `<nav>`Â andÂ `<footer>`Â with tag cloud, copyright
- All links are valid, no 404s

**Sitemap/Robots:**

- `sitemap.xml`: allÂ `/static/**`Â pages
- `robots.txt`:Â `Allow`

**Crawler Tests:**

- Structural HTML parsing
- Recursive internal link traversal
- Sitemap vs DOM discovery comparison
- Relative vs absolute link handling
- Anchor (`#section`) navigation parsing

**Minimum Pages:**Â 30+

---

## **ğŸ”¹ DynamicMaze**

**Purpose:**Â Tests how crawlers handle DOM randomness, structural variation, and inconsistent layout.

**Model:**Â CMS-driven or server-rendered news sites with changing rankings, ads, etc.

**Route:**Â `/dynamic`

**Structure:**

```
/dynamic                 â† Homepage (layout changes every request)
/dynamic/sections/[id]  â† Dynamic section pages (at least 20+)
```

**Key Features:**

- UsesÂ `getServerSideProps`Â to return dynamic content each time
- Varying DOM structure and class names
- Dynamic ads, quotes, section layouts
- Consistent URLs but different page content
- Tricky DOM selectors, changing IDs/classes

**Sitemap/Robots:**

- `sitemap.xml`: none
- `robots.txt`:Â `Disallow: /dynamic`

**Crawler Tests:**

- Structural diff detection between runs
- Selector robustness
- Handling of contradicting robots and actual navigation

**Minimum Pages:**Â 20+

---

## **ğŸ”¹ ClientShadow**

**Purpose:**Â Simulates JavaScript-only rendering. Pages appear empty in HTML and populate content client-side.

**Model:**Â Single Page Applications (SPAs), Next.js CSR-only setups

**Route:**Â `/client-only`

**Structure:**

```
/client-only               â† Top page
/client-only/profile/[id] â† User profiles (at least 25+)
```

**Key Features:**

- Initial HTML is blank or placeholder only
- All content rendered viaÂ `useEffect`
- Hydration-only structure
- No server-side markup
- Internal navigation dynamically injected

**Sitemap/Robots:**

- `sitemap.xml`: available but content mismatch
- `robots.txt`: undefined

**Crawler Tests:**

- JavaScript execution ability
- Visibility of client-injected elements
- JS-only routes, link injection

**Minimum Pages:**Â 25+

---

## **ğŸ”¹ MapTown**

**Purpose:**Â Tests visibility of embedded map data via third-party JS like Google Maps or Leaflet.

**Model:**Â Travel/realtor sites with maps and points of interest

**Route:**Â `/map`

**Structure:**

```
/map                      â† Map-centered homepage
/map/spot/[id]           â† Location detail pages (at least 15+)
```

**Key Features:**

- Uses iframe orÂ `<script>`Â to inject maps
- Google Maps API or OpenStreetMap
- Map-only points with no direct links
- Maps require dummy keys or tokens

**Sitemap/Robots:**

- `sitemap.xml`: includesÂ `/map/spot/**`
- `robots.txt`:Â `Allow`

**Crawler Tests:**

- Extraction of data fromÂ `<iframe>`Â or canvas
- JS-injected content visibility
- Non-link embedded POI discoverability

**Minimum Pages:**Â 15+

---

## **ğŸ”¹ BotWarden**

**Purpose:**Â Evaluates crawler resilience against User-Agent blocking, header filtering, and IP-based traps.

**Model:**Â Login portals, analytic systems, anti-bot strategies

**Route:**Â `/anti-bot`

**Structure:**

```
/anti-bot                â† Homepage (different behavior per UA)
/anti-bot/trap           â† Crawler-only trap page
```

**Key Features:**

- Middleware blocks known bots via User-Agent, IP, Referrer
- Sends 403 or deceptive HTML to bots
- Redirects bots to dead-end loops

**Sitemap/Robots:**

- No sitemap
- `robots.txt`:Â `Disallow: /anti-bot`

**Crawler Tests:**

- User-Agent spoofing
- Middleware rejection handling
- Trap link recognition

**Minimum Pages:**Â 10+

---

## **ğŸ”¹ LinkSpiral**

**Purpose:**Â Tests recursion handling and infinite-link scenarios.

**Model:**Â CMSs with repeated profiles, infinite reply chains

**Route:**Â `/trap/[slug]`

**Structure:**

```
/trap/[slug]            â† Recursive links (at least 50+ slugs)
```

**Key Features:**

- Recursive links between pages
- Loopbacks and redirects
- Random paths linking back into structure

**Sitemap/Robots:**

- Sitemap includes only part of total paths
- `robots.txt`: undefined

**Crawler Tests:**

- Infinite traversal protection
- Loop detection and deduplication

**Minimum Pages:**Â 50+

---

## **ğŸ”¹ BrokenWeb**

**Purpose:**Â Tests crawler error handling for 404s and sitemap-inconsistent URLs.

**Model:**Â Poorly maintained legacy CMS

**Route:**Â `/trap-broken`

**Structure:**

```
/trap-broken             â† Homepage
/trap-broken/pages/[id] â† Pages (half lead to 404)
```

**Key Features:**

- Sitemap lists all pages
- 50% of pages return 404 intentionally
- Some internal links point to missing pages

**Sitemap/Robots:**

- Full sitemap (including dead URLs)
- `robots.txt`:Â `Allow`

**Crawler Tests:**

- 404 response handling
- Retry logic and sitemap trust

**Minimum Pages:**Â 30+ (15+ are broken)

---

## **ğŸ”¹ MetaLie**

**Purpose:**Â Pages with misleading metadataâ€”title and description contradict content.

**Model:**Â SEO spam, phishing, and fake portals

**Route:**Â `/meta-fake`

**Structure:**

```
/meta-fake              â† Landing
/meta-fake/articles/[id] â† Article pages (at least 20+)
```

**Key Features:**

- Titles and meta descriptions are false
- JSON-LD provides misleading schema
- Human vs bot perception divergence

**Sitemap/Robots:**

- Full sitemap
- `robots.txt`:Â `Allow`

**Crawler Tests:**

- Metadata vs content inconsistency
- Schema.org / JSON-LD interpretation

**Minimum Pages:**Â 20+

---

## **ğŸ”¹ NoMapZone**

**Purpose:**Â Has no sitemap or robots.txt, only reachable through in-site link discovery.

**Model:**Â Hobbyist or legacy company pages

**Route:**Â `/no-sitemap`

**Structure:**

```
/no-sitemap             â† Homepage
/no-sitemap/pages/[id] â† Deeply linked pages (at least 40+)
```

**Key Features:**

- Entire site reachable via internal links only
- Multi-level structure, unclear nav paths

**Sitemap/Robots:**

- None

**Crawler Tests:**

- Full graph reconstruction from DOM only
- Link-following accuracy

**Minimum Pages:**Â 40+

---

## **ğŸ”¹ HalfMapSite**

**Purpose:**Â Sitemap exists but only covers half the actual content.

**Model:**Â Partial auto-generated CMS

**Route:**Â `/partial-map`

**Structure:**

```
/partial-map             â† Homepage
/partial-map/pages/[id] â† Pages (at least 40+, sitemap has 20)
```

**Key Features:**

- Only a subset listed in sitemap.xml
- Rest discoverable via internal links

**Sitemap/Robots:**

- Partial sitemap
- `robots.txt`:Â `Allow`

**Crawler Tests:**

- DOM-first vs sitemap-first strategy
- Page discovery completeness

**Minimum Pages:**Â 40+ (only 20 in sitemap)
