# サイトマップとRobots.txtルール

各モックサイトに対して以下のサイトマップ/robotsロジックを実装する必要があります：

## 設定マトリックス

| サイト名        | sitemap.xml | robots.txt  | 期待される結果                                                            |
| -------------- | ----------- | ----------- | ------------------------------------------------------------------------ |
| `StaticLand`   | ✅ 完全     | ✅ 許可     | 完全に表示されクロール可能                                                  |
| `DynamicMaze`  | ❌ なし     | ✅ 禁止     | ルールによってインデックシングから除外                                         |
| `ClientShadow` | ✅ 存在     | ❌ なし     | サイトマップにリストされているがJSなしではコンテンツが不可視                      |
| `BotWarden`    | ❌ なし     | ❌ 禁止     | クローラーが禁止され、サイトマップなし、ミドルウェアで動的拒否                   |
| `BrokenWeb`    | ✅ 存在     | ✅ 許可     | サイトマップリンクが404を返す                                               |
| `HalfMapSite`  | ✅ 部分的   | ✅ 許可     | 部分的なサイトのみリスト、完全な構造を見つけるためにリンクたどりが必要              |
| `NoMapZone`    | ❌ なし     | ❌ なし     | クローラーはすべてのページを有機的に発見しなければならない                      |

## 実装詳細

### サイトマップタイプ

**完全サイトマップ**: サイトのすべての利用可能なページをリスト
- すべての有効なルートを含む
- 正確なlastmod日付を提供
- 適切な優先度値を含む

**部分的サイトマップ**: 意図的に一部の既存ページを省略
- 実際のコンテンツの50%のみをリスト
- クローラーにリンク発見を強制
- クローラーの完全性戦略をテスト

**汚染サイトマップ**: 無効または存在しないURLを含む
- 意図的にデッドリンクを含む
- エラーハンドリング機能をテスト
- リトライロジックを検証

### Robots.txtバリエーション

**すべて許可**: 標準的な許可設定
```
User-agent: *
Allow: /
```

**特定パス禁止**: 特定のパスへのアクセスをブロック
```
User-agent: *
Disallow: /dynamic
Disallow: /anti-bot
```

**robots.txtなし**: ファイルが存在しない
- デフォルトクローラー動作をテスト
- 有機的発見方法を強制

### テストへの影響

各設定は特定のクローラー機能をテストします：
- **コンプライアンス**: robots.txtディレクティブの遵守
- **発見**: サイトマップのガイダンスなしでページを発見
- **エラーハンドリング**: 404やブロークンリンクの管理
- **完全性**: すべての利用可能なコンテンツの発見